{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "# for transformer \n",
    "from transformers import BertModel, BertTokenizer, AdamW\n",
    "import gluonnlp as nlp\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from kobert.utils import get_tokenizer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "# torch library\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "model_path = os.getcwd()\n",
    "#device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition classification and sentiment analysis module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KoBERT Dataloader\n",
    "class KoBERTDataset(Dataset):\n",
    "    def __init__(self, dataset_text, dataset_date, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        \n",
    "        transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "        self.date = [[i] for i in dataset_date]\n",
    "        self.text = [[i] for i in dataset_text]\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset_text]\n",
    "        #self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.text[i],) + (self.date[i],))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return (len(self.sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /nas1/yongk/bigkinds/.cache/kobert_v1.zip\n",
      "using cached model. /nas1/yongk/bigkinds/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n",
      "using cached model. /nas1/yongk/bigkinds/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "# For ESG classification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "_, vocab = get_pytorch_kobert_model()\n",
    "KoBERT_tokenizer = get_tokenizer()\n",
    "BERT_tokenizer = AutoTokenizer.from_pretrained(\"snunlp/KR-FinBert\")\n",
    "tok = nlp.data.BERTSPTokenizer(KoBERT_tokenizer, vocab, lower=False)\n",
    "esg_model = torch.load(model_path + 'esg_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT Classifier\n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert, hidden_size = 768, num_classes = 7, dr_rate = 0.2, params = None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "        \n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        \n",
    "        return attention_mask.float()\n",
    "    \n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        # pooler = pooler.logits\n",
    "\n",
    "        \n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "            \n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sentiment score and labeling\n",
    "from transformers import AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "\n",
    "# load model\n",
    "sent_tokenizer = AutoTokenizer.from_pretrained(\"jaehyeong/koelectra-base-v3-generalized-sentiment-analysis\")\n",
    "sent_model = AutoModelForSequenceClassification.from_pretrained(\"jaehyeong/koelectra-base-v3-generalized-sentiment-analysis\")\n",
    "sentiment_classifier = TextClassificationPipeline(tokenizer=sent_tokenizer, model=sent_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading to clustering\n",
    "\n",
    "empty_list = []\n",
    "root_path = os.getcwd()\n",
    "root_path = root_path + '/2-1. analysis dataset/test/'\n",
    "file_list = os.listdir(root_path)\n",
    "file_list = [file[:-4] for file in file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_hyun', 'test_sin', 'test_lot', 'test_lg', 'test_gs']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f904c9ab0ce4ae7ac4c2cb7fdf4a0d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df31faf182324b738f51d79d296d5079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b961c39d5c420689eb69d3022a90f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67659e0c48e4413386e33889b5358600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0476cd7af763412a8ac78b339ef186e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "for file in file_list:\n",
    "    \n",
    "    \"\"\"\n",
    "    1. test_gs.csv : GS홈쇼핑\n",
    "    2. test_hyun.csv : 현대홈쇼핑\n",
    "    3. test_lot.csv : 롯데홈쇼핑\n",
    "    4. test_sin.csv : 신세계\n",
    "    5. test_lg.csv : LG생활건강\n",
    "\n",
    "    \"\"\"\n",
    "    dir_path = root_path + '{}.csv'.format(file)\n",
    "    df = pd.read_csv(dir_path)\n",
    "    df = df.drop(['Unnamed: 0'], axis=1)\n",
    "    df['text'] = df['title'] + \" \" + df['contents']\n",
    "    df = df.dropna()\n",
    "    \n",
    "    test_esg_df_date = df['date'].tolist()\n",
    "    test_esg_df_text = df['text'].tolist()\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    config & dataloader\n",
    "    \"\"\"\n",
    "    \n",
    "    max_len = 64\n",
    "    batch_size = 64\n",
    "    \n",
    "    dataset = KoBERTDataset(test_esg_df_text, test_esg_df_date, 0, 1, tok, max_len, True, False)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=5)\n",
    "    \n",
    "    \"\"\"\n",
    "    make a analysis dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    # pre_label 이라는 column 2개를 가진 dataframe 생성 \n",
    "    pre_label = pd.DataFrame()\n",
    "    pre_text = pd.DataFrame()\n",
    "    pre_date = pd.DataFrame()\n",
    "\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, text, date) in enumerate(tqdm_notebook(dataloader)):\n",
    "        text_list = []\n",
    "        pre_index_list = []\n",
    "        date_list = []\n",
    "        \n",
    "        text = [text[0][i] for i in range(len(text[0]))]\n",
    "        date = [date[0][i] for i in range(len(date[0]))]\n",
    "        token_ids, valid_length, segment_ids = token_ids.to(device), valid_length.to(device), segment_ids.to(device)\n",
    "                    \n",
    "        logits = esg_model(token_ids, valid_length, segment_ids)\n",
    "        logits = logits.to(torch.float32)\n",
    "        pre_index = torch.argmax(logits, dim=1)\n",
    "        for i in range(pre_index.shape[0]):\n",
    "            text_list.append(text[i])\n",
    "            date_list.append(date[i])\n",
    "            pre_index_list.append(int(pre_index[i].cpu().detach().numpy()))\n",
    "        # pre_index_list를 pre_label dataframe에 추가       \n",
    "        pre_label = pre_label.append(pd.DataFrame(pre_index_list, columns=['pre_label']))\n",
    "        pre_text = pre_text.append(pd.DataFrame(text_list, columns=['text']))\n",
    "        pre_date = pre_date.append(pd.DataFrame(date_list, columns=['date']))\n",
    "        \n",
    "    # pre_label과 pre_text를 concat \n",
    "    pre_df = pd.concat([pre_date, pre_text, pre_label], axis=1)\n",
    "\n",
    "    # pre_df 인덱스 초기화 및 date 올림차순 정렬    \n",
    "    pre_df = pre_df.sort_values(by=['date'], axis=0)\n",
    "    pre_df = pre_df.reset_index(drop=True)\n",
    "    \n",
    "    pre_sentiment_list = []\n",
    "    pre_sent_score_list = []\n",
    "\n",
    "    \n",
    "    for idx, review in enumerate(pre_text['text'].tolist()):\n",
    "        pred = sentiment_classifier(review)\n",
    "        pre_sentiment_list.append(pred[0]['label'])\n",
    "        pre_sent_score_list.append(pred[0]['score'])\n",
    "\n",
    "    pd_sentiment = pd.DataFrame(pre_sentiment_list, columns=['sentiment'])\n",
    "    pd_sent_score = pd.DataFrame(pre_sent_score_list, columns=['sent_score'])\n",
    "\n",
    "    analysis_df = pd.concat([pre_df, pd_sentiment, pd_sent_score], axis=1)\n",
    "    \n",
    "    output_path = root_path +'/output/'\n",
    "    # output 폴더 없으면 만들기\n",
    "\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "        \n",
    "    save_file = output_path + \"{}_analysis.csv\".format(file)\n",
    "\n",
    "    analysis_df.to_csv(save_file, index = False, encoding = 'utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "4d11051c08c60b420006961d4129f6df51068c7dc4cc5e9a33bb00fb5d53a8a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
