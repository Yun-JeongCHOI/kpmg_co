{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import logging\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Data preprocessing\n",
    "import os\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# for transformer \n",
    "from soynlp.normalizer import repeat_normalize\n",
    "from soynlp.normalizer import *\n",
    "from soynlp.noun import NewsNounExtractor\n",
    "from transformers import BertModel, BertTokenizer, AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "import gluonnlp as nlp\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from kobert.utils import get_tokenizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "# torch library\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nas1/yongk/kpmg/BERT_for_finetuning\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>contents</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>피부 철벽같이 보호하는 '철벽녀'…3년 연속 홈쇼핑 화장품 히트상품</td>\n",
       "      <td>이종섭 뷰티피플 대표(50·사진)는 1990년대 중반 지인의 권유로 LG화학(현 L...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>(2020 위기가 기회다)유통가, 온라인에 더 몰린다…배송망·전용 상품 강화 총…</td>\n",
       "      <td>LG생활건강은 세계적인 안무팀과 협업해 온라인 전용 화장품 브랜드 '밀리언뷰티'를 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>[2020 뷰티 전망] 럭셔리 아니면 저가 양극화…맞춤형 화장품 시대 열린다</td>\n",
       "      <td>반면 럭셔리 화장품 '설화수'와 '후'를 내세운 아모레퍼시픽과 LG생활건강 등 '빅...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>실험실 홀랑 태울뻔…日화장품 원료 국산화한 군산 중소기업</td>\n",
       "      <td>“아모레퍼시픽·LG생활건강·콜마 등 국내 화장품 제조사에 보낸 샘플은 긍정적인 답변...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>아모레 안젤라베이비 ‘설화수’ 누른 LG생활건강 이영애 ‘후’, 2020년에도...</td>\n",
       "      <td>2019년 LG생활건강의 럭셔리 브랜드 ‘후’가 선전하며 사상 최대 실적을 기록했다...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        date                                           title  \\\n",
       "0           0  2020-01-01           피부 철벽같이 보호하는 '철벽녀'…3년 연속 홈쇼핑 화장품 히트상품   \n",
       "1           1  2020-01-01   (2020 위기가 기회다)유통가, 온라인에 더 몰린다…배송망·전용 상품 강화 총…   \n",
       "2           2  2020-01-01      [2020 뷰티 전망] 럭셔리 아니면 저가 양극화…맞춤형 화장품 시대 열린다   \n",
       "3           3  2020-01-01                 실험실 홀랑 태울뻔…日화장품 원료 국산화한 군산 중소기업   \n",
       "4           4  2020-01-01  아모레 안젤라베이비 ‘설화수’ 누른 LG생활건강 이영애 ‘후’, 2020년에도...   \n",
       "\n",
       "                                            contents label  \n",
       "0  이종섭 뷰티피플 대표(50·사진)는 1990년대 중반 지인의 권유로 LG화학(현 L...     0  \n",
       "1  LG생활건강은 세계적인 안무팀과 협업해 온라인 전용 화장품 브랜드 '밀리언뷰티'를 ...     0  \n",
       "2  반면 럭셔리 화장품 '설화수'와 '후'를 내세운 아모레퍼시픽과 LG생활건강 등 '빅...     0  \n",
       "3  “아모레퍼시픽·LG생활건강·콜마 등 국내 화장품 제조사에 보낸 샘플은 긍정적인 답변...     0  \n",
       "4  2019년 LG생활건강의 럭셔리 브랜드 ‘후’가 선전하며 사상 최대 실적을 기록했다...     0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Loading to clustering\n",
    "\n",
    "root_path = os.getcwd()\n",
    "print(root_path)\n",
    "dir_path = root_path +'/2-1. analysis dataset/esg_train.csv'\n",
    "\n",
    "df = pd.read_csv(dir_path)\n",
    "#df = df.drop(['Unnamed: 0'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, data_colname):\n",
    "    \"\"\"\n",
    "      tips: csv 데이터를 받아 지정된 column의 내용을 preprocess 합니다.\n",
    "      Args:\n",
    "          data_path : csv데이터의 path\n",
    "          data_colname : 지정할 column명\n",
    "      Returns:\n",
    "          lucy_data : DataFrame\n",
    "    \"\"\"\n",
    "    lucy_data = data\n",
    "\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\"\\(.*\\)|\\s-\\s.*\",\" \" ,regex=True)\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\"\\[.*\\]|\\s-\\s.*\",\" \",regex=True)\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\"\\<.*\\>|\\s-\\s.*\",\" \",regex=True)\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\"무단전재 및 재배포 금지\",\" \",regex=True)\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\"무단 전재 및 재배포 금지\",\" \",regex=True)\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\"©\",\" \",regex=True)\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\"ⓒ\",\" \",regex=True)\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\"저작권자\",\" \",regex=True)\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\".* 기자\", \" \", regex=True) #기자 이름에서 오는 유사도 차단\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\"사진 = .*\", \" \", regex=True) #사진 첨부 문구 삭제\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\"사진=.*\", \" \", regex=True) #사진 첨부 문구 삭제\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace('\\\"', \"\",regex=True)\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\"([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+.[a-zA-Z0-9-.]+)\", \" \", regex=True) #이메일 주소에서 오는 유사도 차단\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\"\\n\",\" \")\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\"\\r\",\" \")\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\"\\t\",\" \")\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace( \"\\’\" , \"\", regex=True)\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\" \")\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\"[ ]{2,}\",\" \",regex=True)\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\"?\",\"\",regex=True)\n",
    "    \n",
    "    return lucy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "#df.contents = df.contents.str[:3]\n",
    "#df.category = df.category.str[:10]\n",
    "# df의 title과 contents를 합쳐서 text column을 만들기\n",
    "df['text'] = df['title'] + df['contents']\n",
    "\n",
    "clean_data = preprocess_data(df, 'text')\n",
    "clean_data['text'] = clean_data['text'].str.replace(\">\",\" \")\n",
    "esg_data = clean_data\n",
    "\n",
    "#esg_data = esg_data.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "# esg_data의 date, text, label column만 가져오기\n",
    "df_1 = esg_data[['date', 'text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data_list = []\n",
    "for date, q, label in zip(esg_data['date'], esg_data['text'], esg_data['label']):\n",
    "    data = []\n",
    "    data.append(date)\n",
    "    data.append(str(q))\n",
    "    label = int(label)\n",
    "    data.append(str(label))\n",
    "    \n",
    "    real_data_list.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argment\n",
    "\n",
    "max_len = 64\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 1\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate = 0.00006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(esg_data, test_size = 0.2, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['text']를 string으로 변환\n",
    "train_df['text'] = train_df['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KoBERT Dataloader\n",
    "class KoBERTDataset(Dataset):\n",
    "    def __init__(self, dataset_text, dataset_date, dataset_label, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        \n",
    "        transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "        self.date = [[i] for i in dataset_date]\n",
    "        self.text = [[i] for i in dataset_text]\n",
    "        self.sentences = [transform([i]) for i in dataset_text]\n",
    "        self.labels = [np.int32(i) for i in dataset_label]\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.text[i],) + (self.date[i],) + (self.labels[i],))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return (len(self.sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /nas1/yongk/kpmg/BERT_for_finetuning/.cache/kobert_v1.zip\n",
      "using cached model. /nas1/yongk/kpmg/BERT_for_finetuning/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n",
      "using cached model. /nas1/yongk/kpmg/BERT_for_finetuning/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "# import pretrained kobert\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "bertmodel, vocab = get_pytorch_kobert_model()\n",
    "KoBERT_tokenizer = get_tokenizer()\n",
    "BERT_tokenizer = AutoTokenizer.from_pretrained(\"snunlp/KR-FinBert\")\n",
    "tok = nlp.data.BERTSPTokenizer(KoBERT_tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KoBERT Dataloader\n",
    "\n",
    "data_train = KoBERTDataset(train_df['text'], train_df['date'], train_df['label'], tok, max_len, True, False)\n",
    "data_test = KoBERTDataset(test_df['text'], test_df['date'], test_df['label'], tok, max_len, True, False)\n",
    "KoBERT_train_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, shuffle=True, num_workers=5)\n",
    "KoBERT_test_loader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, shuffle=True, num_workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert, hidden_size = 768, num_classes = 4, dr_rate = 0.2, params = None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "        \n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        \n",
    "        return attention_mask.float()\n",
    "    \n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        #pooler = pooler.logits\n",
    "        \n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "            \n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "\n",
    "def calc_accuracy(X, Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    \n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training\n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.avg = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = f'{self.name:10s} {self.avg:.8f}'\n",
    "        return fmtstr\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, meters, loader_length, prefix=\"\"):\n",
    "        self.meters = [AverageMeter(i) for i in meters]\n",
    "        self.loader_length = loader_length\n",
    "        self.prefix = prefix\n",
    "    \n",
    "    def reset(self):\n",
    "        for m in self.meters:\n",
    "            m.reset()\n",
    "    \n",
    "    def update(self, values, n=1):\n",
    "        for m, v in zip(self.meters, values):\n",
    "            m.update(v, n)\n",
    "            self.__setattr__(m.name, m.avg)\n",
    "\n",
    "    def display(self, batch_idx, postfix=\"\"):\n",
    "        batch_info = f'[{batch_idx+1:03d}/{self.loader_length:03d}]'\n",
    "        msg = [self.prefix + ' ' + batch_info]\n",
    "        msg += [str(meter) for meter in self.meters]\n",
    "        msg = ' | '.join(msg)\n",
    "\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write(msg + postfix)\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"snunlp/KR-FinBert\")\n",
    "kr_fin = AutoModelForMaskedLM.from_pretrained(\"snunlp/KR-FinBert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClassifier(bertmodel, dr_rate = 0.2).to(device)\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params':[p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params':[p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "t_total = len(KoBERT_train_loader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "#loss_fn = nn.BCEWithLogitsLoss()\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, model, criterion, optimizer, scheduler, device):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        self.best_epoch, self.best_accuracy = 0, 0\n",
    "    \n",
    "    def train(self, train_loader, epoch):\n",
    "        progress = ProgressMeter([\"train_loss\", \"train_acc\"], len(train_loader), prefix=f'EPOCH {epoch:03d}')\n",
    "        self.model.train()\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch_id, (token_ids, valid_length, segment_ids, text, date, label) in enumerate(tqdm_notebook(train_loader)):\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            label = label.unsqueeze(1)\n",
    "            label = label.to(torch.int64)\n",
    "            label = label.squeeze(dim=-1)\n",
    "            label = label.long()\n",
    "            \n",
    "            token_ids, valid_length, segment_ids, label = token_ids.to(self.device), valid_length.to(self.device), segment_ids.to(self.device), label.to(self.device)\n",
    "            logits = self.model(token_ids, valid_length, segment_ids)\n",
    "            logits = logits.to(torch.float32) # torch.size([64, 7])\n",
    "            \n",
    "            loss = self.criterion(logits, label)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            #label = label.cpu().detach().numpy()\n",
    "            #logits = logits.cpu().detach().numpy()\n",
    "            \n",
    "            acc = calc_accuracy(logits, label)\n",
    "            #macro_f1 = f1_loss(label, logits)\n",
    "            \n",
    "            loss = loss.item()\n",
    "            progress.update([loss, acc], n=token_ids.size(0))\n",
    "            if batch_id % 20 == 0:\n",
    "                progress.display(batch_id+1)\n",
    "                \n",
    "        self.scheduler: self.scheduler.step()\n",
    "        finish_time = time.time()\n",
    "        epoch_time = finish_time - start_time\n",
    "        progress.display(batch_id, f' | {epoch_time:.0f}s' + '\\n')\n",
    "        \n",
    "    def validate(self, val_loader, epoch):\n",
    "        progress = ProgressMeter([\"val_loss\", \"val_acc\"], len(val_loader), prefix=f'VALID {epoch:03d}')\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_id, (token_ids, valid_length, segment_ids, text, date, label) in enumerate(tqdm_notebook(val_loader)):\n",
    "                \n",
    "                label = label.unsqueeze(1)\n",
    "                label = label.to(torch.int64)\n",
    "                label = label.squeeze(dim=-1)\n",
    "                label = label.long()\n",
    "                \n",
    "                token_ids, valid_length, segment_ids, label = token_ids.to(self.device), valid_length.to(self.device), segment_ids.to(self.device), label.to(self.device)\n",
    "                logits = self.model(token_ids, valid_length, segment_ids)\n",
    "            \n",
    "                logits = logits.to(torch.float32)\n",
    "                \n",
    "                loss = self.criterion(logits, label)\n",
    "                \n",
    "                acc = calc_accuracy(logits, label)\n",
    "                #macro_f1 = f1_score(label, logits)\n",
    "                progress.update([loss, acc], n=token_ids.size(0))\n",
    "            \n",
    "            progress.display(batch_id, '\\n')\n",
    "            \n",
    "    def test(self, test_loader):\n",
    "        progress = ProgressMeter([\"test_loss\", \"test_acc\"], len(test_loader), prefix=f'TEST')\n",
    "        #ckpt = torch.load(self.output_path + '/ckpt.pt')\n",
    "        #self.model.load_state_dict(ckpt['model_state_dict'])\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_id, (token_ids, valid_length, segment_ids, text, date, label) in enumerate(tqdm_notebook(test_loader)):\n",
    "                label = label.unsqueeze(1)\n",
    "                label = label.to(torch.int64)\n",
    "                label = label.squeeze(dim=-1)\n",
    "                label = label.long()\n",
    "                \n",
    "                token_ids, valid_length, segment_ids, label = token_ids.to(self.device), valid_length.to(self.device), segment_ids.to(self.device), label.to(self.device)\n",
    "                \n",
    "                logits = self.model(token_ids, valid_length, segment_ids)\n",
    "                logits = logits.to(torch.float32)\n",
    "                \n",
    "                loss = self.criterion(logits, label)\n",
    "                \n",
    "                acc = calc_accuracy(logits, label)\n",
    "                #macro_f1 = f1_score(label, logits, zero_division='warn', average='macro')\n",
    "                progress.update([loss, acc], n=token_ids.size(0))\n",
    "            progress.display(batch_id, '\\n')\n",
    "            torch.save(self.model, root_path + '/2-1. analysis dataset/esg_class_finetuning.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer config define\n",
    "trainer = Trainer(model, loss_fn, optimizer, scheduler, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "283cfaaf937d45edbef3e1e3aa68c2c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 000 [244/244] | train_loss 0.44685975 | train_acc  0.85877446 | 65s\n"
     ]
    }
   ],
   "source": [
    "# train & test\n",
    "for epoch in range(num_epochs):\n",
    "        trainer.train(KoBERT_train_loader, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e728ea9b2046d8844b05ec5b976d32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST [061/061] | test_loss  0.31383690 | test_acc   0.89915319\n"
     ]
    }
   ],
   "source": [
    "# test traiend kobert\n",
    "test_esg = trainer.test(KoBERT_test_loader)\n",
    "test_esg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "esg_class_model = torch.load(root_path + '/2-1. analysis dataset/esg_class_finetuning.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b30c46d48348a19e75950cbb61c696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pre_label 이라는 column 2개를 가진 dataframe 생성 \n",
    "pre_label = pd.DataFrame()\n",
    "pre_text = pd.DataFrame()\n",
    "pre_date = pd.DataFrame()\n",
    "\n",
    "for batch_id, (token_ids, valid_length, segment_ids, text, date) in enumerate(tqdm_notebook(KoBERT_test_loader)):\n",
    "\n",
    "    text_list = []\n",
    "    pre_index_list = []\n",
    "    date_list = []\n",
    "    \n",
    "    text = list(text)\n",
    "    date = list(date)\n",
    "    token_ids, valid_length, segment_ids = token_ids.to(device), valid_length.to(device), segment_ids.to(device)\n",
    "                \n",
    "    logits = esg_class_model(token_ids, valid_length, segment_ids)\n",
    "    logits = logits.to(torch.float32)\n",
    "    pre_index = torch.argmax(logits, dim=1)\n",
    "    for i in range(pre_index.shape[0]):\n",
    "        text_list.append(text[i])\n",
    "        date_list.append(date[i])\n",
    "        pre_index_list.append(int(pre_index[i].cpu().detach().numpy()))\n",
    "    # pre_index_list를 pre_label dataframe에 추가       \n",
    "    pre_label = pre_label.append(pd.DataFrame(pre_index_list, columns=['pre_label']))\n",
    "    pre_text = pre_text.append(pd.DataFrame(text_list, columns=['text']))\n",
    "    pre_date = pre_date.append(pd.DataFrame(date_list, columns=['date']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_label과 pre_text를 concat \n",
    "pre_df = pd.concat([pre_text, pre_label], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5bee82f1aa4c2a7f2905b2bb19edba8ddd243e499f23a2172eda27cb9b9b8967"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
