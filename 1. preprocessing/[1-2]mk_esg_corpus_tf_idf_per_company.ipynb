{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install soynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf 추출 모듈 import\n",
    "\n",
    "%run module_pdf_extractor.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미 추출되어 있는 공시데이터 텍스트 가져오기\n",
    "\n",
    "path = './1-2. ESG report/'\n",
    "file_list = os.listdir(path+'output/')\n",
    "file_list = [file for file in file_list if file.startswith(\"ESG_text\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 공시 보고서별 e/s/g 페이지 번호 미리 저장\n",
    "# e/s/g 순서\n",
    "\n",
    "dict = {\n",
    "    'GSretail_2023':[range(7,18), range(18,34),range(2,7)],\n",
    "    'hyundaehomeshopping_2023' : [range(6,16), range(16,32), range(2,6)],\n",
    "    'LGlife_2023' : [range(5,22), range(22,53), range(2,5)],\n",
    "    'lottehi_2023' : [range(5,14), range(14,32), range(2,5)],\n",
    "    'sinsegye_2023' : [range(6,18), range(18,35), range(2,6)],\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e/s/g별 텍스트 모두 모아서 각각 텍스트 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = ''\n",
    "s = ''\n",
    "g = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "\n",
    "for file in file_list:\n",
    "    name = file.replace('ESG_text_','').replace('.csv','')\n",
    "    data = json_to_csv(path, '', name)\n",
    "    globals()[f'{name}_e'] = ''\n",
    "    globals()[f'{name}_s'] = ''\n",
    "    globals()[f'{name}_g'] = ''\n",
    "\n",
    "\n",
    "    for j in range(len(data[\"elements\"])):\n",
    "        if 'Text' in data[\"elements\"][j]:\n",
    "            page = data[\"elements\"][j]['Page']\n",
    "            if page in dict[name][0]:\n",
    "                    globals()[f'{name}_e'] = globals()[f'{name}_e'] + data[\"elements\"][j]['Text'] + ' '\n",
    "            elif page in dict[name][1]:\n",
    "                    globals()[f'{name}_s'] = globals()[f'{name}_s'] + data[\"elements\"][j]['Text'] + ' '\n",
    "            elif page in dict[name][2]:\n",
    "                    globals()[f'{name}_g'] = globals()[f'{name}_g'] + data[\"elements\"][j]['Text'] + ' '\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    p = path +'output/'+f'{name}_e_doublespacelinecorpus.txt'\n",
    "    paths.append(p)\n",
    "    file = open(p, 'w', encoding = 'utf-8')\n",
    "    file.write(globals()[f'{name}_e']+' ')\n",
    "    file.close()\n",
    "    p = path +'output/'+f'{name}_s_doublespacelinecorpus.txt'\n",
    "    paths.append(p)\n",
    "    file = open(p, 'w', encoding = 'utf-8')\n",
    "    file.write(globals()[f'{name}_s']+' ')\n",
    "    file.close()\n",
    "    p = path +'output/'+f'{name}_g_doublespacelinecorpus.txt'\n",
    "    paths.append(p)\n",
    "    file = open(p, 'w', encoding = 'utf-8')\n",
    "    file.write(globals()[f'{name}_g']+' ')\n",
    "    file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트 파일로 저장된 e/s/g별 문서 불러와서 명사 추출\n",
    "\n",
    "Soynlp의 LRNounExtractor_v2 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1298 from 527 sents. mem=0.137 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2103, mem=0.137 Gb\n",
      "[Noun Extractor] batch prediction was completed for 401 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 153 -> 153\n",
      "[Noun Extractor] postprocessing ignore_features : 153 -> 147\n",
      "[Noun Extractor] postprocessing ignore_NJ : 147 -> 147\n",
      "[Noun Extractor] 147 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.137 Gb                    \n",
      "[Noun Extractor] 33.48 % eojeols are covered\n",
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2503 from 1041 sents. mem=0.137 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4900, mem=0.137 Gb\n",
      "[Noun Extractor] batch prediction was completed for 781 words\n",
      "[Noun Extractor] checked compounds. discovered 71 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 350 -> 350\n",
      "[Noun Extractor] postprocessing ignore_features : 350 -> 343\n",
      "[Noun Extractor] postprocessing ignore_NJ : 343 -> 343\n",
      "[Noun Extractor] 343 nouns (71 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.137 Gb                    \n",
      "[Noun Extractor] 46.71 % eojeols are covered\n",
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1118 from 431 sents. mem=0.137 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2170, mem=0.137 Gb\n",
      "[Noun Extractor] batch prediction was completed for 354 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 153 -> 152\n",
      "[Noun Extractor] postprocessing ignore_features : 152 -> 149\n",
      "[Noun Extractor] postprocessing ignore_NJ : 149 -> 149\n",
      "[Noun Extractor] 149 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.137 Gb                    \n",
      "[Noun Extractor] 40.88 % eojeols are covered\n",
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1089 from 516 sents. mem=0.137 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2135, mem=0.137 Gb\n",
      "[Noun Extractor] batch prediction was completed for 330 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 160 -> 160\n",
      "[Noun Extractor] postprocessing ignore_features : 160 -> 154\n",
      "[Noun Extractor] postprocessing ignore_NJ : 154 -> 154\n",
      "[Noun Extractor] 154 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.137 Gb                    \n",
      "[Noun Extractor] 42.90 % eojeols are covered\n",
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2280 from 1173 sents. mem=0.137 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4834, mem=0.137 Gb\n",
      "[Noun Extractor] batch prediction was completed for 710 words\n",
      "[Noun Extractor] checked compounds. discovered 65 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 363 -> 361\n",
      "[Noun Extractor] postprocessing ignore_features : 361 -> 352\n",
      "[Noun Extractor] postprocessing ignore_NJ : 352 -> 352\n",
      "[Noun Extractor] 352 nouns (65 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.137 Gb                    \n",
      "[Noun Extractor] 49.36 % eojeols are covered\n",
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 661 from 388 sents. mem=0.137 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1284, mem=0.137 Gb\n",
      "[Noun Extractor] batch prediction was completed for 222 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 90 -> 89\n",
      "[Noun Extractor] postprocessing ignore_features : 89 -> 86\n",
      "[Noun Extractor] postprocessing ignore_NJ : 86 -> 86\n",
      "[Noun Extractor] 86 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.137 Gb                    \n",
      "[Noun Extractor] 34.58 % eojeols are covered\n",
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2201 from 1118 sents. mem=0.137 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4191, mem=0.137 Gb\n",
      "[Noun Extractor] batch prediction was completed for 618 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 272 -> 272\n",
      "[Noun Extractor] postprocessing ignore_features : 272 -> 266\n",
      "[Noun Extractor] postprocessing ignore_NJ : 266 -> 266\n",
      "[Noun Extractor] 266 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.137 Gb                    \n",
      "[Noun Extractor] 47.10 % eojeols are covered\n",
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3632 from 1408 sents. mem=0.137 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=7737, mem=0.137 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1138 words\n",
      "[Noun Extractor] checked compounds. discovered 81 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 554 -> 554\n",
      "[Noun Extractor] postprocessing ignore_features : 554 -> 541\n",
      "[Noun Extractor] postprocessing ignore_NJ : 541 -> 541\n",
      "[Noun Extractor] 541 nouns (81 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.137 Gb                    \n",
      "[Noun Extractor] 55.49 % eojeols are covered\n",
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 566 from 140 sents. mem=0.137 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=888, mem=0.137 Gb\n",
      "[Noun Extractor] batch prediction was completed for 198 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 77 -> 76\n",
      "[Noun Extractor] postprocessing ignore_features : 76 -> 73\n",
      "[Noun Extractor] postprocessing ignore_NJ : 73 -> 73\n",
      "[Noun Extractor] 73 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.137 Gb                    \n",
      "[Noun Extractor] 42.45 % eojeols are covered\n",
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 759 from 147 sents. mem=0.137 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1175, mem=0.137 Gb\n",
      "[Noun Extractor] batch prediction was completed for 262 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 97 -> 97\n",
      "[Noun Extractor] postprocessing ignore_features : 97 -> 95\n",
      "[Noun Extractor] postprocessing ignore_NJ : 95 -> 95\n",
      "[Noun Extractor] 95 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.137 Gb                    \n",
      "[Noun Extractor] 37.79 % eojeols are covered\n",
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2241 from 765 sents. mem=0.137 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4211, mem=0.137 Gb\n",
      "[Noun Extractor] batch prediction was completed for 724 words\n",
      "[Noun Extractor] checked compounds. discovered 39 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 330 -> 330\n",
      "[Noun Extractor] postprocessing ignore_features : 330 -> 325\n",
      "[Noun Extractor] postprocessing ignore_NJ : 325 -> 325\n",
      "[Noun Extractor] 325 nouns (39 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.137 Gb                    \n",
      "[Noun Extractor] 44.95 % eojeols are covered\n",
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 574 from 267 sents. mem=0.137 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=991, mem=0.137 Gb\n",
      "[Noun Extractor] batch prediction was completed for 200 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 75 -> 73\n",
      "[Noun Extractor] postprocessing ignore_features : 73 -> 72\n",
      "[Noun Extractor] postprocessing ignore_NJ : 72 -> 72\n",
      "[Noun Extractor] 72 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.137 Gb                    \n",
      "[Noun Extractor] 27.55 % eojeols are covered\n",
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1120 from 346 sents. mem=0.137 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1967, mem=0.137 Gb\n",
      "[Noun Extractor] batch prediction was completed for 376 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 161 -> 161\n",
      "[Noun Extractor] postprocessing ignore_features : 161 -> 158\n",
      "[Noun Extractor] postprocessing ignore_NJ : 158 -> 158\n",
      "[Noun Extractor] 158 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.137 Gb                    \n",
      "[Noun Extractor] 43.67 % eojeols are covered\n",
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1903 from 706 sents. mem=0.137 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3822, mem=0.137 Gb\n",
      "[Noun Extractor] batch prediction was completed for 675 words\n",
      "[Noun Extractor] checked compounds. discovered 39 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 290 -> 290\n",
      "[Noun Extractor] postprocessing ignore_features : 290 -> 285\n",
      "[Noun Extractor] postprocessing ignore_NJ : 285 -> 285\n",
      "[Noun Extractor] 285 nouns (39 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.137 Gb                    \n",
      "[Noun Extractor] 48.51 % eojeols are covered\n",
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 630 from 256 sents. mem=0.137 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1090, mem=0.137 Gb\n",
      "[Noun Extractor] batch prediction was completed for 221 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 95 -> 95\n",
      "[Noun Extractor] postprocessing ignore_features : 95 -> 90\n",
      "[Noun Extractor] postprocessing ignore_NJ : 90 -> 90\n",
      "[Noun Extractor] 90 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.137 Gb                    \n",
      "[Noun Extractor] 38.35 % eojeols are covered\n"
     ]
    }
   ],
   "source": [
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "from soynlp.noun import LRNounExtractor_v2\n",
    "\n",
    "for path in paths:\n",
    "    corpus_path = path\n",
    "    sents = DoublespaceLineCorpus(corpus_path, iter_sent=True)\n",
    "\n",
    "    noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
    "    noun_extractor.train(sents)\n",
    "    nouns.append(noun_extractor.extract())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈도수만큼 해당 명사를 리스트에 저장\n",
    "\n",
    "for i, n in enumerate(nouns):\n",
    "    globals()[f\"keys_{file_list[i//3]}_{i%3}\"] = []\n",
    "    for w, v in n.items():\n",
    "        for j in range(int(v[0])):\n",
    "            globals()[f\"keys_{file_list[i//3]}_{i%3}\"].append(w)\n",
    "    globals()[f\"keys_{file_list[i//3]}_{i%3}\"] = [k for k in locals()[f\"keys_{file_list[i//3]}_{i%3}\"] if k.isalpha()]\n",
    "    globals()[f\"keys_{file_list[i//3]}_{i%3}\"] = [k for k in locals()[f\"keys_{file_list[i//3]}_{i%3}\"] if not re.match(r\"\\d{4}년\", k)]\n",
    "    globals()[f\"keys_{file_list[i//3]}_{i%3}\"] = [k for k in locals()[f\"keys_{file_list[i//3]}_{i%3}\"] if not re.search(r'현대홈쇼핑|GS|롯데|신세계|LG', k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(len(file_list)):\n",
    "    globals()[f'{file_list[n]}_e'] = [i for i in globals()[f'keys_{file_list[n]}_0'] if len(i)>2 and not i.isdigit()]\n",
    "    globals()[f'{file_list[n]}_s'] = [i for i in globals()[f'keys_{file_list[n]}_1'] if len(i)>2 and not i.isdigit()]\n",
    "    globals()[f'{file_list[n]}_g'] = [i for i in globals()[f'keys_{file_list[n]}_2'] if len(i)>2 and not i.isdigit()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for n in range(len(file_list)):\n",
    "    docs.append(globals()[f'{file_list[n]}_e'])\n",
    "    docs.append(globals()[f'{file_list[n]}_s'])\n",
    "    docs.append(globals()[f'{file_list[n]}_g'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(t, d):\n",
    "  return d.count(t)\n",
    "\n",
    "def idf(t):\n",
    "  c = 0\n",
    "  for doc in docs:\n",
    "    c += t in doc\n",
    "  return np.log(len(docs)/(c+sys.float_info.min))\n",
    "\n",
    "def tfidf(t, d):\n",
    "  return tf(t,d)* idf(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "corps = []\n",
    "for n in range(len(file_list)):\n",
    "    corp = pd.DataFrame(globals()[f'{file_list[n]}_e'],columns= ['word'])\n",
    "    corps.append((f'{file_list[n]}',corp,globals()[f'{file_list[n]}_e']))\n",
    "    corp = pd.DataFrame(globals()[f'{file_list[n]}_s'],columns= ['word'])\n",
    "    corps.append((f'{file_list[n]}',corp,globals()[f'{file_list[n]}_s']))\n",
    "    corp = pd.DataFrame(globals()[f'{file_list[n]}_g'],columns= ['word'])\n",
    "    corps.append((f'{file_list[n]}',corp,globals()[f'{file_list[n]}_g']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 계산 및 내림차순으로 정렬\n",
    "\n",
    "for c in corps:\n",
    "    for i, item in c[1].iterrows():\n",
    "        c[1].loc[i,'tf-idf'] = tfidf(item['word'],c[2])\n",
    "    c[1].drop_duplicates(inplace = True)\n",
    "    c[1].sort_values(by = 'tf-idf', ascending=False, inplace=True)\n",
    "    c[1].reset_index(inplace=True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESG_text_GSretail_2023.csv 38\n",
      "ESG_text_GSretail_2023.csv 133\n",
      "ESG_text_GSretail_2023.csv 52\n",
      "ESG_text_hyundaehomeshopping_2023.csv 49\n",
      "ESG_text_hyundaehomeshopping_2023.csv 148\n",
      "ESG_text_hyundaehomeshopping_2023.csv 26\n",
      "ESG_text_LGlife_2023.csv 75\n",
      "ESG_text_LGlife_2023.csv 197\n",
      "ESG_text_LGlife_2023.csv 20\n",
      "ESG_text_lottehi_2023.csv 24\n",
      "ESG_text_lottehi_2023.csv 111\n",
      "ESG_text_lottehi_2023.csv 23\n",
      "ESG_text_sinsegye_2023.csv 50\n",
      "ESG_text_sinsegye_2023.csv 107\n",
      "ESG_text_sinsegye_2023.csv 24\n"
     ]
    }
   ],
   "source": [
    "for c in corps:\n",
    "    print(c[0], len(c[1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF 점수가 1보다 큰 단어만 코퍼스로 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSretail_2023corpus 1805\n",
      "GSretail_2023corpus 1805\n",
      "GSretail_2023corpus 1805\n",
      "hyundaehomeshopping_2023corpus 1798\n",
      "hyundaehomeshopping_2023corpus 1798\n",
      "hyundaehomeshopping_2023corpus 1798\n",
      "LGlife_2023corpus 2357\n",
      "LGlife_2023corpus 2357\n",
      "LGlife_2023corpus 2357\n"
     ]
    }
   ],
   "source": [
    "esg = ('e','s','g','e','s','g','e','s','g')\n",
    "# corp = ((e_corpus,10), (s_corpus,10), (g_corpus,10))\n",
    "for c in corps:\n",
    "    name = c[0].replace('ESG_text_','').replace('.csv','')\n",
    "    globals()[f'{name}corpus'] = pd.DataFrame(columns=['word', 'label', 'tf-idf'])\n",
    "\n",
    "for i, c in zip(esg, corps):\n",
    "    name = c[0].replace('ESG_text_','').replace('.csv','')\n",
    "    # corpus = pd.concat([corpus,c[0][c[0]['tf-idf']>c[1]]])\n",
    "    globals()[f'{name}corpus'] = pd.concat([globals()[f'{c[0]}corpus'],c[1]])\n",
    "    globals()[f'{name}corpus'] = globals()[f'{c[0]}corpus'].reset_index(drop=True)\n",
    "    globals()[f'{name}corpus'].loc[len(globals()[f'{c[0]}corpus'])-1,'label'] = i\n",
    "    print(f'{name}corpus', len(globals()[f'{c[0]}corpus']))\n",
    "\n",
    "for file in file_list:\n",
    "    name = file.replace('ESG_text_','').replace('.csv','')\n",
    "    # 기업명 관련 키워드 제거\n",
    "    # globals()[f'{name}corpus'] = globals()[f'{name}corpus'][~globals()[f'{name}corpus']['word'].str.contains(r\"현대홈쇼핑|GS|롯데|신세계|LG\", regex=True)]\n",
    "    # print(len(globals()[f'{name}corpus']))\n",
    "    globals()[f'{name}corpus'] = globals()[f'{name}corpus'].fillna(method = 'bfill')\n",
    "    globals()[f'{name}corpus'] = globals()[f'{name}corpus'].dropna()\n",
    "    globals()[f'{name}corpus'] = globals()[f'{name}corpus'].drop_duplicates()\n",
    "    globals()[f'{name}corpus'] = globals()[f'{name}corpus'].reset_index(drop = True)\n",
    "    path = './1-2. ESG report/'\n",
    "    if not os.path.isdir(path + 'output/'):\n",
    "        os.mkdir(path + 'output/')\n",
    "    globals()[f'{name}corpus'].to_csv(path + 'output/'+ 'corpus_esg_keyword_v4_' + name +'.csv', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec8dee48dbbcaba4ab33c211cc3985cb6ada4ac8ebaeb72c3fb14197585c2120"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
