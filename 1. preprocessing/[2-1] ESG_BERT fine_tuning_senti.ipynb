{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import logging\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Data preprocessing\n",
    "import os\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# for transformer \n",
    "from soynlp.normalizer import repeat_normalize\n",
    "from soynlp.normalizer import *\n",
    "from soynlp.noun import NewsNounExtractor\n",
    "from transformers import BertModel, BertTokenizer, AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "import gluonnlp as nlp\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from kobert.utils import get_tokenizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "# torch library\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nas1/yongk/kpmg/BERT_for_finetuning\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>가구보러 왔다가 책도 읽고 가지요신세계 리빙앤라이프스타일 브랜드 까사미아는 지난해 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>가구보러 왔다가 책도 읽고 가지요신세계 리빙앤라이프스타일 브랜드 까사미아는 지난해 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>재계 신년사 미래 고객 디지털 혁신 화두 그룹과 롯데 신세계 그룹은 고객 중심에 방...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>신년사 미래 고객 디지털 혁신 화두 그룹과 롯데 신세계 그룹은 고객 중심에 방점을 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>고체 식초 발포정 리아퐁 독보적인 기술력과 제품력으로 엔타스 면세점 입점고체 식초 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                               text  sentiment\n",
       "0  2020-01-01  가구보러 왔다가 책도 읽고 가지요신세계 리빙앤라이프스타일 브랜드 까사미아는 지난해 ...          1\n",
       "1  2020-01-01  가구보러 왔다가 책도 읽고 가지요신세계 리빙앤라이프스타일 브랜드 까사미아는 지난해 ...          1\n",
       "2  2020-01-02  재계 신년사 미래 고객 디지털 혁신 화두 그룹과 롯데 신세계 그룹은 고객 중심에 방...          0\n",
       "3  2020-01-02  신년사 미래 고객 디지털 혁신 화두 그룹과 롯데 신세계 그룹은 고객 중심에 방점을 ...          0\n",
       "4  2020-01-02  고체 식초 발포정 리아퐁 독보적인 기술력과 제품력으로 엔타스 면세점 입점고체 식초 ...          0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Loading to clustering\n",
    "\n",
    "root_path = os.getcwd()\n",
    "print(root_path)\n",
    "dir_path = root_path +'/2-1. analysis dataset/esg_data_for_sentiment.csv'\n",
    "\n",
    "df = pd.read_csv(dir_path)\n",
    "#df = df.drop(['Unnamed: 0'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, data_colname):\n",
    "    \"\"\"\n",
    "      tips: csv 데이터를 받아 지정된 column의 내용을 preprocess 합니다.\n",
    "      Args:\n",
    "          data_path : csv데이터의 path\n",
    "          data_colname : 지정할 column명\n",
    "      Returns:\n",
    "          lucy_data : DataFrame\n",
    "    \"\"\"\n",
    "    lucy_data = data\n",
    "\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\"\\(.*\\)|\\s-\\s.*\",\" \" ,regex=True)\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\"\\[.*\\]|\\s-\\s.*\",\" \",regex=True)\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\"\\<.*\\>|\\s-\\s.*\",\" \",regex=True)\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\"무단전재 및 재배포 금지\",\" \",regex=True)\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\"무단 전재 및 재배포 금지\",\" \",regex=True)\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\"©\",\" \",regex=True)\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\"ⓒ\",\" \",regex=True)\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\"저작권자\",\" \",regex=True)\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\".* 기자\", \" \", regex=True) #기자 이름에서 오는 유사도 차단\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\"사진 = .*\", \" \", regex=True) #사진 첨부 문구 삭제\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\"사진=.*\", \" \", regex=True) #사진 첨부 문구 삭제\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace('\\\"', \"\",regex=True)\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\"([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+.[a-zA-Z0-9-.]+)\", \" \", regex=True) #이메일 주소에서 오는 유사도 차단\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\"\\n\",\" \")\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\"\\r\",\" \")\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\"\\t\",\" \")\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace( \"\\’\" , \"\", regex=True)\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\" \")\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\"[ ]{2,}\",\" \",regex=True)\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\"?\",\"\",regex=True)\n",
    "    \n",
    "    return lucy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "# df의 title과 contents column을 합쳐서 text column을 만듭니다.\n",
    "#df['text'] = df['title'] + df['contents']\n",
    "clean_data = preprocess_data(df, 'text')\n",
    "clean_data['text'] = clean_data['text'].str.replace(\">\",\" \")\n",
    "esg_data = clean_data\n",
    "\n",
    "#esg_data = esg_data.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "# esg_data의 date, text, label column만 가져오기\n",
    "esg_data = esg_data[['date', 'text', 'sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "esg_data\n",
    "# sentiment label을 string으로\n",
    "esg_data['sentiment'] = esg_data['sentiment'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3504\n",
       "1    2769\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# esg_data 개수 세기\n",
    "esg_data['sentiment'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_2(data, data_colname):\n",
    "    \"\"\"\n",
    "      tips: csv 데이터를 받아 지정된 column의 내용을 preprocess 합니다.\n",
    "      Args:\n",
    "          data_path : csv데이터의 path\n",
    "          data_colname : 지정할 column명\n",
    "      Returns:\n",
    "          lucy_data : DataFrame\n",
    "    \"\"\"\n",
    "    lucy_data = data\n",
    "\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\"?\",\"\",regex=True)\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\".\",\"0\",regex=True)\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\"20\",\"2\",regex=True)\n",
    "    lucy_data[data_colname] = lucy_data[data_colname].str.replace(\"44\",\"4\",regex=True)\n",
    "    \n",
    "    return lucy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .str accessor with string values!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/nas1/yongk/kpmg/BERT_for_finetuning/ESG_BERT fine_tuning copy.ipynb 셀 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B165.132.193.40/nas1/yongk/kpmg/BERT_for_finetuning/ESG_BERT%20fine_tuning%20copy.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m esg_data \u001b[39m=\u001b[39m preprocess_data_2(esg_data, \u001b[39m'\u001b[39;49m\u001b[39msentiment\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m/nas1/yongk/kpmg/BERT_for_finetuning/ESG_BERT fine_tuning copy.ipynb 셀 11\u001b[0m in \u001b[0;36mpreprocess_data_2\u001b[0;34m(data, data_colname)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B165.132.193.40/nas1/yongk/kpmg/BERT_for_finetuning/ESG_BERT%20fine_tuning%20copy.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B165.132.193.40/nas1/yongk/kpmg/BERT_for_finetuning/ESG_BERT%20fine_tuning%20copy.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m  tips: csv 데이터를 받아 지정된 column의 내용을 preprocess 합니다.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B165.132.193.40/nas1/yongk/kpmg/BERT_for_finetuning/ESG_BERT%20fine_tuning%20copy.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B165.132.193.40/nas1/yongk/kpmg/BERT_for_finetuning/ESG_BERT%20fine_tuning%20copy.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m      lucy_data : DataFrame\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B165.132.193.40/nas1/yongk/kpmg/BERT_for_finetuning/ESG_BERT%20fine_tuning%20copy.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B165.132.193.40/nas1/yongk/kpmg/BERT_for_finetuning/ESG_BERT%20fine_tuning%20copy.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m lucy_data \u001b[39m=\u001b[39m data\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B165.132.193.40/nas1/yongk/kpmg/BERT_for_finetuning/ESG_BERT%20fine_tuning%20copy.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m lucy_data[data_colname] \u001b[39m=\u001b[39m lucy_data[data_colname]\u001b[39m.\u001b[39;49mstr\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m?\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,regex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B165.132.193.40/nas1/yongk/kpmg/BERT_for_finetuning/ESG_BERT%20fine_tuning%20copy.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m lucy_data[data_colname] \u001b[39m=\u001b[39m lucy_data[data_colname]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m,regex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B165.132.193.40/nas1/yongk/kpmg/BERT_for_finetuning/ESG_BERT%20fine_tuning%20copy.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m lucy_data[data_colname] \u001b[39m=\u001b[39m lucy_data[data_colname]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m20\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39m2\u001b[39m\u001b[39m\"\u001b[39m,regex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/bigkind/lib/python3.8/site-packages/pandas/core/generic.py:5907\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5900\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   5901\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[1;32m   5902\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[1;32m   5903\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[1;32m   5904\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5905\u001b[0m ):\n\u001b[1;32m   5906\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[0;32m-> 5907\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "File \u001b[0;32m~/anaconda3/envs/bigkind/lib/python3.8/site-packages/pandas/core/accessor.py:183\u001b[0m, in \u001b[0;36mCachedAccessor.__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m     \u001b[39m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessor\n\u001b[0;32m--> 183\u001b[0m accessor_obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_accessor(obj)\n\u001b[1;32m    184\u001b[0m \u001b[39m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[39m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39m# NDFrame\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name, accessor_obj)\n",
      "File \u001b[0;32m~/anaconda3/envs/bigkind/lib/python3.8/site-packages/pandas/core/strings/accessor.py:182\u001b[0m, in \u001b[0;36mStringMethods.__init__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, data) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39marrays\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstring_\u001b[39;00m \u001b[39mimport\u001b[39;00m StringDtype\n\u001b[0;32m--> 182\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_dtype \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate(data)\n\u001b[1;32m    183\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_categorical \u001b[39m=\u001b[39m is_categorical_dtype(data\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    184\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_string \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(data\u001b[39m.\u001b[39mdtype, StringDtype)\n",
      "File \u001b[0;32m~/anaconda3/envs/bigkind/lib/python3.8/site-packages/pandas/core/strings/accessor.py:236\u001b[0m, in \u001b[0;36mStringMethods._validate\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    233\u001b[0m inferred_dtype \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39minfer_dtype(values, skipna\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    235\u001b[0m \u001b[39mif\u001b[39;00m inferred_dtype \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m allowed_types:\n\u001b[0;32m--> 236\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCan only use .str accessor with string values!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    237\u001b[0m \u001b[39mreturn\u001b[39;00m inferred_dtype\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can only use .str accessor with string values!"
     ]
    }
   ],
   "source": [
    "esg_data = preprocess_data_2(esg_data, 'sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data_list = []\n",
    "for date, q, label in zip(esg_data['date'], esg_data['text'], esg_data['sentiment']):\n",
    "    data = []\n",
    "    data.append(date)\n",
    "    data.append(str(q))\n",
    "    label = int(label)\n",
    "    data.append(str(label))\n",
    "    \n",
    "    real_data_list.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argment\n",
    "\n",
    "max_len = 128\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 2\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate = 0.00003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(esg_data, test_size = 0.2, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['text']를 string으로 변환\n",
    "train_df['text'] = train_df['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KoBERT Dataloader\n",
    "class KoBERTDataset(Dataset):\n",
    "    def __init__(self, dataset_text, dataset_date, dataset_label, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        \n",
    "        transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "        self.date = [[i] for i in dataset_date]\n",
    "        self.text = [[i] for i in dataset_text]\n",
    "        self.sentences = [transform([i]) for i in dataset_text]\n",
    "        self.labels = [np.int32(i) for i in dataset_label]\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.text[i],) + (self.date[i],) + (self.labels[i],))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return (len(self.sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /nas1/yongk/kpmg/BERT_for_finetuning/.cache/kobert_v1.zip\n",
      "using cached model. /nas1/yongk/kpmg/BERT_for_finetuning/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n",
      "using cached model. /nas1/yongk/kpmg/BERT_for_finetuning/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "# import pretrained kobert\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "bertmodel, vocab = get_pytorch_kobert_model()\n",
    "KoBERT_tokenizer = get_tokenizer()\n",
    "BERT_tokenizer = AutoTokenizer.from_pretrained(\"snunlp/KR-FinBert\")\n",
    "tok = nlp.data.BERTSPTokenizer(KoBERT_tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KoBERT Dataloader\n",
    "\n",
    "data_train = KoBERTDataset(train_df['text'], train_df['date'], train_df['sentiment'], tok, max_len, True, False)\n",
    "data_test = KoBERTDataset(test_df['text'], test_df['date'], test_df['sentiment'], tok, max_len, True, False)\n",
    "KoBERT_train_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, shuffle=True, num_workers=5)\n",
    "KoBERT_test_loader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, shuffle=True, num_workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier_senti(nn.Module):\n",
    "    def __init__(self, bert, hidden_size = 768, num_classes = 2, dr_rate = 0.2, params = None):\n",
    "        super(BERTClassifier_senti, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "        \n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        \n",
    "        return attention_mask.float()\n",
    "    \n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        #pooler = pooler.logits\n",
    "        \n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "            \n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "\n",
    "def calc_accuracy(X, Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    \n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training\n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.avg = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = f'{self.name:10s} {self.avg:.8f}'\n",
    "        return fmtstr\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, meters, loader_length, prefix=\"\"):\n",
    "        self.meters = [AverageMeter(i) for i in meters]\n",
    "        self.loader_length = loader_length\n",
    "        self.prefix = prefix\n",
    "    \n",
    "    def reset(self):\n",
    "        for m in self.meters:\n",
    "            m.reset()\n",
    "    \n",
    "    def update(self, values, n=1):\n",
    "        for m, v in zip(self.meters, values):\n",
    "            m.update(v, n)\n",
    "            self.__setattr__(m.name, m.avg)\n",
    "\n",
    "    def display(self, batch_idx, postfix=\"\"):\n",
    "        batch_info = f'[{batch_idx+1:03d}/{self.loader_length:03d}]'\n",
    "        msg = [self.prefix + ' ' + batch_info]\n",
    "        msg += [str(meter) for meter in self.meters]\n",
    "        msg = ' | '.join(msg)\n",
    "\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write(msg + postfix)\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"snunlp/KR-FinBert\")\n",
    "kr_fin = AutoModelForMaskedLM.from_pretrained(\"snunlp/KR-FinBert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClassifier_senti(bertmodel, dr_rate = 0.2).to(device)\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params':[p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params':[p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "t_total = len(KoBERT_train_loader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "#loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, model, criterion, optimizer, scheduler, device):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        self.best_epoch, self.best_accuracy = 0, 0\n",
    "    \n",
    "    def train(self, train_loader, epoch):\n",
    "        progress = ProgressMeter([\"train_loss\", \"train_acc\"], len(train_loader), prefix=f'EPOCH {epoch:03d}')\n",
    "        self.model.train()\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch_id, (token_ids, valid_length, segment_ids, text, date, label) in enumerate(tqdm_notebook(train_loader)):\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            label = label.unsqueeze(1)\n",
    "            label = label.to(torch.int64)\n",
    "            label = label.squeeze(dim=-1)\n",
    "            label = label.long()\n",
    "            \n",
    "            token_ids, valid_length, segment_ids, label = token_ids.to(self.device), valid_length.to(self.device), segment_ids.to(self.device), label.to(self.device)\n",
    "            logits = self.model(token_ids, valid_length, segment_ids)\n",
    "            logits = logits.to(torch.float32) # torch.size([64, 7])\n",
    "            \n",
    "            loss = self.criterion(logits, label)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            #label = label.cpu().detach().numpy()\n",
    "            #logits = logits.cpu().detach().numpy()\n",
    "            \n",
    "            acc = calc_accuracy(logits, label)\n",
    "            #macro_f1 = f1_loss(label, logits)\n",
    "            \n",
    "            loss = loss.item()\n",
    "            progress.update([loss, acc], n=token_ids.size(0))\n",
    "            if batch_id % 20 == 0:\n",
    "                progress.display(batch_id+1)\n",
    "                \n",
    "        self.scheduler: self.scheduler.step()\n",
    "        finish_time = time.time()\n",
    "        epoch_time = finish_time - start_time\n",
    "        progress.display(batch_id, f' | {epoch_time:.0f}s' + '\\n')\n",
    "        \n",
    "    def validate(self, val_loader, epoch):\n",
    "        progress = ProgressMeter([\"val_loss\", \"val_acc\"], len(val_loader), prefix=f'VALID {epoch:03d}')\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_id, (token_ids, valid_length, segment_ids, text, date, label) in enumerate(tqdm_notebook(val_loader)):\n",
    "                \n",
    "                label = label.unsqueeze(1)\n",
    "                label = label.to(torch.int64)\n",
    "                label = label.squeeze(dim=-1)\n",
    "                label = label.long()\n",
    "                \n",
    "                token_ids, valid_length, segment_ids, label = token_ids.to(self.device), valid_length.to(self.device), segment_ids.to(self.device), label.to(self.device)\n",
    "                logits = self.model(token_ids, valid_length, segment_ids)\n",
    "            \n",
    "                logits = logits.to(torch.float32)\n",
    "                \n",
    "                loss = self.criterion(logits, label)\n",
    "                \n",
    "                acc = calc_accuracy(logits, label)\n",
    "                #macro_f1 = f1_score(label, logits)\n",
    "                progress.update([loss, acc], n=token_ids.size(0))\n",
    "            \n",
    "            progress.display(batch_id, '\\n')\n",
    "            \n",
    "    def test(self, test_loader):\n",
    "        progress = ProgressMeter([\"test_loss\", \"test_acc\"], len(test_loader), prefix=f'TEST')\n",
    "        #ckpt = torch.load(self.output_path + '/ckpt.pt')\n",
    "        #self.model.load_state_dict(ckpt['model_state_dict'])\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_id, (token_ids, valid_length, segment_ids, text, date, label) in enumerate(tqdm_notebook(test_loader)):\n",
    "                label = label.unsqueeze(1)\n",
    "                label = label.to(torch.int64)\n",
    "                label = label.squeeze(dim=-1)\n",
    "                label = label.long()\n",
    "                \n",
    "                token_ids, valid_length, segment_ids, label = token_ids.to(self.device), valid_length.to(self.device), segment_ids.to(self.device), label.to(self.device)\n",
    "                \n",
    "                logits = self.model(token_ids, valid_length, segment_ids)\n",
    "                logits = logits.to(torch.float32)\n",
    "                \n",
    "                loss = self.criterion(logits, label)\n",
    "                \n",
    "                acc = calc_accuracy(logits, label)\n",
    "                #macro_f1 = f1_score(label, logits, zero_division='warn', average='macro')\n",
    "                progress.update([loss, acc], n=token_ids.size(0))\n",
    "            progress.display(batch_id, '\\n')\n",
    "            torch.save(self.model, root_path + '/2-1. analysis dataset/esg_sent_finetuning.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer config define\n",
    "trainer = Trainer(model, loss_fn, optimizer, scheduler, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16cd6f60835b4c7a9e54b9ca30c75c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 000 [079/079] | train_loss 0.42571477 | train_acc  0.79453966 | 34s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e16a899432f41e5a9a284852dce0dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 001 [079/079] | train_loss 0.16940879 | train_acc  0.94400159 | 32s\n"
     ]
    }
   ],
   "source": [
    "# train & test\n",
    "for epoch in range(num_epochs):\n",
    "        trainer.train(KoBERT_train_loader, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3449cfcacf1c46a69532a78c163dca79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST [020/020] | test_loss  0.14257644 | test_acc   0.95697211\n"
     ]
    }
   ],
   "source": [
    "# test traiend kobert\n",
    "test_esg = trainer.test(KoBERT_test_loader)\n",
    "test_esg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "esg_sent_model = torch.load(root_path + '/2-1. analysis dataset/esg_sent_finetuning.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b30c46d48348a19e75950cbb61c696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pre_label 이라는 column 2개를 가진 dataframe 생성 \n",
    "pre_label = pd.DataFrame()\n",
    "pre_text = pd.DataFrame()\n",
    "pre_date = pd.DataFrame()\n",
    "\n",
    "for batch_id, (token_ids, valid_length, segment_ids, text, date) in enumerate(tqdm_notebook(KoBERT_test_loader)):\n",
    "\n",
    "    text_list = []\n",
    "    pre_index_list = []\n",
    "    date_list = []\n",
    "    \n",
    "    text = list(text)\n",
    "    date = list(date)\n",
    "    token_ids, valid_length, segment_ids = token_ids.to(device), valid_length.to(device), segment_ids.to(device)\n",
    "                \n",
    "    logits = esg_sent_model(token_ids, valid_length, segment_ids)\n",
    "    logits = logits.to(torch.float32)\n",
    "    pre_index = torch.argmax(logits, dim=1)\n",
    "    for i in range(pre_index.shape[0]):\n",
    "        text_list.append(text[i])\n",
    "        date_list.append(date[i])\n",
    "        pre_index_list.append(int(pre_index[i].cpu().detach().numpy()))\n",
    "    # pre_index_list를 pre_label dataframe에 추가       \n",
    "    pre_label = pre_label.append(pd.DataFrame(pre_index_list, columns=['pre_label']))\n",
    "    pre_text = pre_text.append(pd.DataFrame(text_list, columns=['text']))\n",
    "    pre_date = pre_date.append(pd.DataFrame(date_list, columns=['date']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_label과 pre_text를 concat \n",
    "pre_df = pd.concat([pre_text, pre_label], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5bee82f1aa4c2a7f2905b2bb19edba8ddd243e499f23a2172eda27cb9b9b8967"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
