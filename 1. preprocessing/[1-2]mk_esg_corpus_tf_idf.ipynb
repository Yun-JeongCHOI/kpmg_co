{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install soynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf 추출 모듈 import\n",
    "\n",
    "%run module_pdf_extractor.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미 추출되어 있는 공시데이터 텍스트 가져오기\n",
    "\n",
    "path = './1-2. ESG report/'\n",
    "file_list = os.listdir(path+'output/')\n",
    "file_list = [file for file in file_list if file.startswith(\"ESG_text\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 공시 보고서별 e/s/g 페이지 번호 미리 저장\n",
    "# e/s/g 순서\n",
    "\n",
    "dict = {\n",
    "    'GSretail_2023':[range(7,18), range(18,34),range(2,7)],\n",
    "    'hyundaehomeshopping_2023' : [range(6,16), range(16,32), range(2,6)],\n",
    "    'LGlife_2023' : [range(5,22), range(22,53), range(2,5)],\n",
    "    'lottehi_2023' : [range(5,14), range(14,32), range(2,5)],\n",
    "    'sinsegye_2023' : [range(6,18), range(18,35), range(2,6)],\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e/s/g별 텍스트 모두 모아서 각각 텍스트 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e/s/g별 토큰 리스트 만들기\n",
    "# e = []\n",
    "# s = []\n",
    "# g = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = ''\n",
    "s = ''\n",
    "g = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in file_list:\n",
    "    name = file.replace('ESG_text_','').replace('.csv','')\n",
    "    data = json_to_csv(path, '', name)\n",
    "\n",
    "    for j in range(len(data[\"elements\"])):\n",
    "        if 'Text' in data[\"elements\"][j]:\n",
    "            page = data[\"elements\"][j]['Page']\n",
    "            if page in dict[name][0]:\n",
    "                    e = e + data[\"elements\"][j]['Text'] + ' '\n",
    "            elif page in dict[name][1]:\n",
    "                    s = s + data[\"elements\"][j]['Text'] + ' '\n",
    "            elif page in dict[name][2]:\n",
    "                    g = g + data[\"elements\"][j]['Text'] + ' '\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('e_doublespacelinecorpus.txt', 'w', encoding = 'utf-8')\n",
    "file.write(e+' ')\n",
    "file.close()\n",
    "file = open('s_doublespacelinecorpus.txt', 'w', encoding = 'utf-8')\n",
    "file.write(s+' ')\n",
    "file.close()\n",
    "file = open('g_doublespacelinecorpus.txt', 'w', encoding = 'utf-8')\n",
    "file.write(g+' ')\n",
    "file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트 파일로 저장된 e/s/g별 문서 불러와서 명사 추출\n",
    "\n",
    "Soynlp의 LRNounExtractor_v2 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = ['e_doublespacelinecorpus.txt', 's_doublespacelinecorpus.txt', 'g_doublespacelinecorpus.txt']\n",
    "nouns = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5152 from 2653 sents. mem=0.150 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=11571, mem=0.150 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1370 words\n",
      "[Noun Extractor] checked compounds. discovered 95 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 667 -> 667\n",
      "[Noun Extractor] postprocessing ignore_features : 667 -> 660\n",
      "[Noun Extractor] postprocessing ignore_NJ : 660 -> 660\n",
      "[Noun Extractor] 660 nouns (95 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.151 Gb                    \n",
      "[Noun Extractor] 53.06 % eojeols are covered\n",
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 9207 from 5093 sents. mem=0.150 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=25504, mem=0.151 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2581 words\n",
      "[Noun Extractor] checked compounds. discovered 574 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1642 -> 1636\n",
      "[Noun Extractor] postprocessing ignore_features : 1636 -> 1619\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1619 -> 1619\n",
      "[Noun Extractor] 1619 nouns (574 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.154 Gb                    \n",
      "[Noun Extractor] 61.97 % eojeols are covered\n",
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2510 from 1482 sents. mem=0.151 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6423, mem=0.151 Gb\n",
      "[Noun Extractor] batch prediction was completed for 731 words\n",
      "[Noun Extractor] checked compounds. discovered 78 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 386 -> 380\n",
      "[Noun Extractor] postprocessing ignore_features : 380 -> 371\n",
      "[Noun Extractor] postprocessing ignore_NJ : 371 -> 371\n",
      "[Noun Extractor] 371 nouns (78 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.151 Gb                    \n",
      "[Noun Extractor] 51.30 % eojeols are covered\n"
     ]
    }
   ],
   "source": [
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "from soynlp.noun import LRNounExtractor_v2\n",
    "\n",
    "for path in paths:\n",
    "    corpus_path = path\n",
    "    sents = DoublespaceLineCorpus(corpus_path, iter_sent=True)\n",
    "\n",
    "    noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
    "    noun_extractor.train(sents)\n",
    "    nouns.append(noun_extractor.extract())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈도수만큼 해당 명사를 리스트에 저장\n",
    "\n",
    "for i, n in enumerate(nouns):\n",
    "    globals()[f\"keys_{i}\"] = []\n",
    "    for w, v in n.items():\n",
    "        for j in range(int(v[0])):\n",
    "            locals()[f\"keys_{i}\"].append(w)\n",
    "    globals()[f\"keys_{i}\"] = [k for k in locals()[f\"keys_{i}\"] if k.isalpha()]\n",
    "    globals()[f\"keys_{i}\"] = [k for k in locals()[f\"keys_{i}\"] if not re.match(r\"\\d{4}년\", k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단순 띄어쓰기 단위 토큰 추출\n",
    "\n",
    "# for file in file_list:\n",
    "#     name = file.replace('ESG_text_','').replace('.csv','')\n",
    "#     data = json_to_csv(path, '', name)\n",
    "\n",
    "#     for j in range(len(data[\"elements\"])):\n",
    "#         if 'Text' in data[\"elements\"][j]:\n",
    "#             page = data[\"elements\"][j]['Page']\n",
    "#             if page in dict[name][0]:\n",
    "#                     e = e + (data[\"elements\"][j]['Text']).split(' ')\n",
    "#             elif page in dict[name][1]:\n",
    "#                     s = s + (data[\"elements\"][j]['Text']).split(' ')\n",
    "#             elif page in dict[name][2]:\n",
    "#                     g = g + (data[\"elements\"][j]['Text']).split(' ')\n",
    "#             else:\n",
    "#                 pass\n",
    "#         else:\n",
    "#             pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = [i for i in keys_0 if len(i)>2 and not i.isdigit()]\n",
    "s = [i for i in keys_1 if len(i)>2 and not i.isdigit()]\n",
    "g = [i for i in keys_2 if len(i)>2 and not i.isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1679 4029 991\n"
     ]
    }
   ],
   "source": [
    "# tf-idf 계산 전 전체 명사 수\n",
    "print(len(e), len(s), len(g))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [e, s, g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(t, d):\n",
    "  return d.count(t)\n",
    "\n",
    "def idf(t):\n",
    "  df = 0\n",
    "  for doc in docs:\n",
    "    df += t in doc\n",
    "  return np.log(len(docs)/(df+sys.float_info.min))\n",
    "\n",
    "def tfidf(t, d):\n",
    "  return tf(t,d)* idf(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_corpus = pd.DataFrame(e,columns= ['word'])\n",
    "s_corpus = pd.DataFrame(s,columns= ['word'])\n",
    "g_corpus = pd.DataFrame(g,columns= ['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 계산 및 내림차순으로 정렬\n",
    "corpus = [(e_corpus,e), (s_corpus,s), (g_corpus, g)]\n",
    "\n",
    "for c in corpus:\n",
    "    for i, item in c[0].iterrows():\n",
    "        c[0].loc[i,'tf-idf'] = tfidf(item['word'],c[1])\n",
    "    c[0].drop_duplicates(inplace = True)\n",
    "    c[0].sort_values(by = 'tf-idf', ascending=False, inplace=True)\n",
    "    c[0].reset_index(inplace=True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n",
      "910\n",
      "160\n"
     ]
    }
   ],
   "source": [
    "for c in corpus:\n",
    "    print(len(c[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF 점수가 1보다 큰 단어만 코퍼스로 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "esg = ('e','s','g')\n",
    "corp = (e_corpus, s_corpus, g_corpus)\n",
    "corpus = pd.DataFrame(columns=['word', 'label', 'tf-idf'])\n",
    "\n",
    "for i, c in zip(esg, corp):\n",
    "    corpus = pd.concat([corpus,c[c['tf-idf']>1]])\n",
    "    corpus.reset_index(drop=True)\n",
    "    corpus.loc[len(corpus)-1,'label'] = i\n",
    "\n",
    "corpus.fillna(method = 'bfill', inplace=True)\n",
    "corpus.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기업명 관련 키워드 제거\n",
    "\n",
    "corpus = corpus[~corpus['word'].str.contains(r\"현대홈쇼핑|GS|롯데|신세계|LG\", regex=True)]\n",
    "corpus.reset_index(drop = True, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './1-2. ESG report/'\n",
    "if not os.path.isdir(path + 'output/'):\n",
    "    os.mkdir(path + 'output/')\n",
    "corpus.to_csv(path + 'output/'+'corpus_esg_keyword_v2'+'.csv', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec8dee48dbbcaba4ab33c211cc3985cb6ada4ac8ebaeb72c3fb14197585c2120"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
